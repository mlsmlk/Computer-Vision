{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oT2dgJC-_Lw"
      },
      "source": [
        "\n",
        "# **ECSE  415 Assignment 3**\n",
        "#Melis Malki - 260775809\n",
        "##November 02, 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57QFSeKj_DHR"
      },
      "source": [
        "##**Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olTpTuFI8U_X",
        "outputId": "338bec29-00c7-4c2e-ee9a-c8b120c3d5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Loading train and test images\n",
        "data = np.load('/content/drive/My Drive/ECSE 415/Assignment_3/flower_subset.npz')\n",
        "train_images =data['train_images']\n",
        "train_labels = data['train_labels']\n",
        "test_images = data['test_images']\n",
        "test_labels = data['test_labels']\n",
        "\n",
        "temp_train = []\n",
        "temp_test = []\n",
        "\n",
        "#Resizing images for HoG\n",
        "for i in range(len(train_images)):\n",
        "  temp_train.append(cv2.resize(train_images[i],(64,64)))\n",
        "train_images = np.array(temp_train)\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "  temp_test.append(cv2.resize(test_images[i],(64,64)))\n",
        "test_images = np.array(temp_test)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OyovAtP_nsO"
      },
      "source": [
        "##**SVM and RF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DK-p6yDRoAA"
      },
      "source": [
        "###HoG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp0wvfPHRpac"
      },
      "source": [
        "# HoG Features Function\n",
        "def hog_fetaures(dataset):\n",
        "  ##HoG Object\n",
        "  img_size = (64, 64)\n",
        "  cell_size = (8, 8)\n",
        "  block_size = (4, 4)\n",
        "  numberOfBins = 4\n",
        "  hog = cv2.HOGDescriptor(_winSize=(img_size[1] // cell_size[1] * cell_size[1],\n",
        "                                    img_size[0] // cell_size[0] * cell_size[0]),\n",
        "                          _blockSize=(block_size[1] * cell_size[1],\n",
        "                                      block_size[0] * cell_size[0]),\n",
        "                          _blockStride=(cell_size[1], cell_size[0]),\n",
        "                          _cellSize=(cell_size[1], cell_size[0]),\n",
        "                          _nbins=numberOfBins)\n",
        "\n",
        "  features = []\n",
        "  for i in range(dataset.shape[0]):\n",
        "    features.append(hog.compute((dataset[i]*255).astype(np.uint8)).reshape(1, -1))\n",
        "  features = np.vstack(features)\n",
        "  return features\n",
        "\n",
        "# Compute HoG\n",
        "train_hog_features = hog_fetaures(train_images)\n",
        "test_hog_features = hog_fetaures(test_images)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjv2Gl3-QivC"
      },
      "source": [
        "###SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anAknyncp9uN",
        "outputId": "5abc0770-9e0b-4d30-dfef-559887ec78d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Define SVM Classifier\n",
        "SVM_classifier = svm.SVC(gamma='auto', C=1) \n",
        "\n",
        "#Fit SVM classifier on Hog and label of the training images\n",
        "SVM_classifier.fit(train_hog_features, train_labels)\n",
        "\n",
        "#Predict\n",
        "SVM_prediction = SVM_classifier.predict(test_hog_features)\n",
        "print(\"Actual Label:\", test_labels)\n",
        "print(\"Predicted Label:\", SVM_prediction)\n",
        "print('Accuracy: {}%'.format(accuracy_score(test_labels, SVM_prediction)*100)) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Label: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
            " 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7\n",
            " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8]\n",
            "Predicted Label: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Accuracy: 11.11111111111111%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w29ITgdcrJW4"
      },
      "source": [
        "### SVM with More Than 50% Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvAEleKAC_a",
        "outputId": "4cf12304-b9e5-4b06-a440-317dbeb030df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Define new SVM Classifier for 50% accuracy\n",
        "SVM_classifier = svm.SVC(gamma='auto', C=100) \n",
        "\n",
        "#Fit SVM classifier on Hog and label of the training images\n",
        "SVM_classifier.fit(train_hog_features, train_labels)\n",
        "\n",
        "#Predict\n",
        "SVM_prediction = SVM_classifier.predict(test_hog_features)\n",
        "print(\"Actual Label:\", test_labels)\n",
        "print(\"Predicted Label:\", SVM_prediction)\n",
        "print('Accuracy: {}%'.format(accuracy_score(test_labels, SVM_prediction)*100)) "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Label: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
            " 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7\n",
            " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8]\n",
            "Predicted Label: [0 0 0 0 0 3 8 0 0 0 0 7 5 1 1 1 1 6 1 1 1 2 3 2 2 2 2 2 2 2 7 3 3 2 3 3 3\n",
            " 1 3 6 4 4 4 4 4 4 2 4 0 4 0 2 5 6 5 5 1 1 4 5 1 1 1 7 6 6 6 1 5 6 1 1 0 0\n",
            " 7 3 1 7 4 1 0 8 8 1 8 4 0 8 0 4]\n",
            "Accuracy: 55.55555555555556%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpR4lsoRQdaA"
      },
      "source": [
        "###RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZNW8ia8rVlm",
        "outputId": "ab262d7d-d42e-4789-8bc3-f2995429fdef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Define RF Classifier\n",
        "RF_classifier = RandomForestClassifier(n_estimators=10, max_depth=5, criterion='entropy')\n",
        "\n",
        "#Fit RF classifier on Hog and label of the training images\n",
        "RF_classifier.fit(train_hog_features, train_labels)\n",
        "\n",
        "# Predict\n",
        "RF_prediction = RF_classifier.predict(test_hog_features)\n",
        "print(\"Actual Label:\", test_labels)\n",
        "print(\"Predicted Label:\", RF_prediction)\n",
        "print('Random Forest Accuracy: {}%'.format(accuracy_score(test_labels, RF_prediction)*100))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Label: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
            " 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7\n",
            " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8]\n",
            "Predicted Label: [0 4 4 1 4 3 4 4 4 4 0 0 6 1 1 4 4 6 4 5 4 2 2 2 2 2 2 2 2 3 4 3 1 2 1 2 3\n",
            " 7 3 6 4 8 0 4 4 4 5 4 4 4 4 2 5 3 5 1 6 4 4 1 6 6 6 1 5 6 2 1 1 1 1 0 3 1\n",
            " 4 6 1 4 4 4 0 4 8 1 8 8 1 8 4 4]\n",
            "Random Forest Accuracy: 34.44444444444444%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLnU20kxE1E"
      },
      "source": [
        "### RF with More Than 50% Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Kgu2duQcxT",
        "outputId": "6ab436b0-13c1-4f79-a0db-a50b051a66df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Define RF Classifier with more than 50% Accuracy\n",
        "RF_classifier = RandomForestClassifier(n_estimators=70, max_depth=30, criterion='entropy')\n",
        "\n",
        "#Fit RF classifier on Hog and label of the training images\n",
        "RF_classifier.fit(train_hog_features, train_labels)\n",
        "\n",
        "# Predict\n",
        "RF_prediction = RF_classifier.predict(test_hog_features)\n",
        "print(\"Actual Label:\", test_labels)\n",
        "print(\"Predicted Label:\", RF_prediction)\n",
        "print('Random Forest Accuracy: {}%'.format(accuracy_score(test_labels, RF_prediction)*100))\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Label: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
            " 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7\n",
            " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8]\n",
            "Predicted Label: [0 0 0 4 0 3 8 0 0 0 1 1 5 7 1 0 1 1 4 8 1 2 3 2 2 2 2 2 2 1 4 3 1 3 1 3 3\n",
            " 8 3 6 4 4 4 4 4 4 4 4 4 4 8 2 5 1 8 7 6 4 1 1 1 1 2 7 2 6 6 1 6 6 1 1 0 3\n",
            " 7 7 7 7 4 4 0 8 8 1 8 4 1 8 0 1]\n",
            "Random Forest Accuracy: 52.22222222222223%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTaohpuSfhO"
      },
      "source": [
        "##Comparision of  SVM and RF classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwOgrJ0Tmv-W",
        "outputId": "fbdf9929-298b-48ec-aaaf-b00032a8cfce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "random_numbers = [1,10,15,30,57]\n",
        "\n",
        "#RF\n",
        "for num in random_numbers:\n",
        "  RF_classifier = RandomForestClassifier(n_estimators=10, max_depth=5, random_state= num,criterion='entropy')\n",
        "  RF_classifier.fit(train_hog_features, train_labels)\n",
        "  RF_prediction = RF_classifier.predict(test_hog_features)\n",
        "  print('Random Forest Accuracy (random_state =' +str(num) +'): {}%'.format(accuracy_score(test_labels, RF_prediction)*100))\n",
        "#SVM\n",
        "for num in random_numbers:\n",
        "  SVM_classifier = svm.SVC(gamma='auto', C=1,random_state=num) \n",
        "  SVM_classifier.fit(train_hog_features, train_labels)\n",
        "  SVM_prediction = SVM_classifier.predict(test_hog_features)\n",
        "  print('SVM Accuracy (random_state =' +str(num) +'): {}%'.format(accuracy_score(test_labels, SVM_prediction)*100)) \n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy (random_state =1): 36.666666666666664%\n",
            "Random Forest Accuracy (random_state =10): 35.55555555555556%\n",
            "Random Forest Accuracy (random_state =15): 37.77777777777778%\n",
            "Random Forest Accuracy (random_state =30): 27.77777777777778%\n",
            "Random Forest Accuracy (random_state =57): 35.55555555555556%\n",
            "SVM Accuracy (random_state =1): 11.11111111111111%\n",
            "SVM Accuracy (random_state =10): 11.11111111111111%\n",
            "SVM Accuracy (random_state =15): 11.11111111111111%\n",
            "SVM Accuracy (random_state =30): 11.11111111111111%\n",
            "SVM Accuracy (random_state =57): 11.11111111111111%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIpcWEgIp2m0"
      },
      "source": [
        "When SVM and RF classifiers are tested with 5 different random states, it is observed that SVM results are more reliable. The reason is SVM results are more stable and robust, thus more accurate compared to RF. RF classifier's accuracy rate differs significantly so it gives unstable results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRGNwpD-SxTr"
      },
      "source": [
        "##**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLgkKQd0SRVL",
        "outputId": "021054c6-bfa3-4740-a647-7c7fd8400418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#File Uploading\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#CUDA Device check\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
        "                             transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size, shuffle=True,num_workers=2)\n",
        "\n",
        "test_set = torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
        "                             transform=transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=batch_size, shuffle=False,num_workers=2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivcCftczi13R",
        "outputId": "21df2328-7ab6-456b-db90-265b6c526b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Implement CNN with layers\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,kernel_size=(3,3))\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=(3,3))\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64,kernel_size=(3,3))\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64,kernel_size=(3,3)) \n",
        "        self.fc1 = nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = torch.flatten(x,1,3)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.to(device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6JYLSLmUGq"
      },
      "source": [
        "#SGD Optimizer\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "#Train CNN\n",
        "for epoch in range(10): \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jFvcFpwbe7n"
      },
      "source": [
        "#Save the training\n",
        "PATH = './mnist_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcJoFckbivP",
        "outputId": "ce9bd08b-9a4b-4828-bf48-e916d6b3d7a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Predict Test Data\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 96 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}